{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eaa13d1b-2668-49c6-8c10-31cc18f12e9f",
   "metadata": {},
   "source": [
    "# Pre-processing for cfDNA fragment center counts\n",
    "- Aggregate counts into 2000 bp intervals\n",
    "- z-score normalization\n",
    "- Smoothing with Whittaker smoothing followed by Guassian filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a65b8b5-cd1a-4006-b71b-f6cde02b6960",
   "metadata": {},
   "source": [
    "## Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679c99f4-461d-41fa-ba02-cf3436475d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from whittaker_eilers import WhittakerSmoother\n",
    "import scipy.ndimage\n",
    "from scipy.stats import zscore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658d9069-08d2-4e7a-9464-216cbefa541d",
   "metadata": {},
   "source": [
    "## Smoothing function (Whittaker and Gaussian smooting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724f3c2a-b5ec-402c-bcb1-ec96462e0a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_fragment_centers(fragment_center_array, lmbda=1000, sigma=30):\n",
    "    \"\"\"Applies Whittaker smoothing followed by Gaussian smoothing.\"\"\"\n",
    "    whittaker_smoother = WhittakerSmoother(\n",
    "        lmbda=lmbda, order=2, data_length=len(fragment_center_array))\n",
    "    smoothed_fragment_centers = np.array(whittaker_smoother.smooth(fragment_center_array))\n",
    "    return scipy.ndimage.gaussian_filter1d(smoothed_fragment_centers, sigma)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c717f4eb-020c-4f68-97d6-b921254e09de",
   "metadata": {},
   "source": [
    "## Function to load, normalize, smooth and save cfDNA fragment center counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4646168e-5e5a-49a3-91f4-ebd97b210c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sample(sample_path):\n",
    "    \"\"\"Processes a single sample by loading, normalizing, and smoothing fragment counts.\"\"\"\n",
    "    matrix_file = os.path.join(sample_path, 'mapped_counts', f\"{os.path.basename(sample_path)}_all_counts.bed\")\n",
    "    if not os.path.exists(matrix_file):\n",
    "        print(f\"Missing file: {matrix_file}, skipping...\")\n",
    "        return\n",
    "    \n",
    "    # Load and format cfDNA raw fragment counts\n",
    "    df_matrix = pd.read_csv(matrix_file, sep=\"\\t\", header=None, names=['chrom', 'start', 'end', 'count'])\n",
    "    counts = df_matrix['count'].values\n",
    "    \n",
    "    # Define interval size and reshape counts\n",
    "    interval_size = 2000\n",
    "    reshaped_counts = counts.reshape(-1, interval_size)\n",
    "    df_counts_split = pd.DataFrame(reshaped_counts)\n",
    "    \n",
    "    # Extract start and end positions for each interval\n",
    "    regions = df_matrix.iloc[::interval_size, :3].reset_index(drop=True)\n",
    "    regions['end'] = regions['start'] + interval_size\n",
    "    \n",
    "    # Ensure regions match row count\n",
    "    if len(regions) != df_counts_split.shape[0]:\n",
    "        raise ValueError(f\"Mismatch in reshaped data and BED regions for {sample_path}\")\n",
    "    \n",
    "    # Assign region indices\n",
    "    df_counts_split.index = regions.apply(lambda row: f\"{row['chrom']}:{row['start']}-{row['end']}\", axis=1)\n",
    "    \n",
    "    # Z-score normalization\n",
    "    df_zscore_rows = df_counts_split.apply(zscore, axis=1).fillna(0)\n",
    "    \n",
    "    # Apply smoothing\n",
    "    df_smoothed = df_zscore_rows.apply(lambda row: smooth_fragment_centers(row.values), axis=1)\n",
    "    df_smoothed = pd.DataFrame(df_smoothed.tolist(), columns=df_zscore_rows.columns, index=df_zscore_rows.index)\n",
    "    \n",
    "    # Save as pickle\n",
    "    output_file = os.path.join(sample_path, f\"{os.path.basename(sample_path)}_smoothed.pkl\")\n",
    "    df_smoothed.to_pickle(output_file)\n",
    "    print(f\"Processed and saved: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2572dc72-859b-4cb4-aece-cc077ed71dd3",
   "metadata": {},
   "source": [
    "## Main function for processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72da3795-3b47-424f-a136-1c6dfb4de1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Main function to process all samples in the base directory.\"\"\"\n",
    "    base_dir = \"/mnt/DATA3/daniel/project/02_cfDNA_preprocessing/data/03_intersect_mapped/cfDNA_healthy_new/\"\n",
    "    \n",
    "    # Iterate over each sample directory\n",
    "    for sample in os.listdir(base_dir):\n",
    "        sample_path = os.path.join(base_dir, sample)\n",
    "        if os.path.isdir(sample_path):\n",
    "            print(f\"Processing: {sample}\")\n",
    "            process_sample(sample_path)\n",
    "    \n",
    "    print(\"Processing complete for all samples.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69420df3-cb46-4c75-87d5-79511110ca87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
